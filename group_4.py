# -*- coding: utf-8 -*-
"""Group-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nlcHl1sBABGNjgx0Y0jRLpoVm9QWH6FD
"""

# loading necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

# from google.colab import files
# uploaded = files.upload()

data = pd.read_csv("/content/drive/My Drive/Data Science/Project/airline_passenger_satisfaction.csv", index_col=['ID'])
data.head()

"""#### Data Understanding"""

# Continuous Features Report
import warnings

def build_continuous_features_report(data_df):

    """Build tabular report for continuous features"""

    stats = {
        "Count": len,
        "Miss %": lambda df: df.isna().sum() / len(df) * 100,
        "Card.": lambda df: df.nunique(),
        "Min": lambda df: df.min(),
        "1st Qrt.": lambda df: df.quantile(0.25),
        "Mean": lambda df: df.mean(),
        "Median": lambda df: df.median(),
        "3rd Qrt": lambda df: df.quantile(0.75),
        "Max": lambda df: df.max(),
        "Std. Dev.": lambda df: df.std(),
    }

    contin_feat_names = data_df.select_dtypes("number").columns
    continuous_data_df = data_df[contin_feat_names]

    report_df = pd.DataFrame(index=contin_feat_names, columns=stats.keys())

    for stat_name, fn in stats.items():
        # NOTE: ignore warnings for empty features
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            report_df[stat_name] = fn(continuous_data_df)

    return report_df

build_continuous_features_report(data)

binwidth=2
data.iloc[:,1:].hist(bins=11, figsize=(25,20), color='darkred')
plt.show()

# Categorical Features Report
data.describe(exclude=['number'])

binwidth = 2
data.iloc[:, 1:].hist(bins=11, figsize=(25, 20), color='darkred', edgecolor='black', alpha=0.7)

# Get the axes of the subplots
axes = plt.gcf().get_axes()

# Iterate through each subplot and convert the histogram to a bar plot
for ax in axes:
    # Get the histogram data
    heights, bins, _ = ax.hist([], bins=11, color='darkred', edgecolor='black', alpha=0.7)

    # Calculate the bin centers
    bin_centers = 0.5 * (bins[:-1] + bins[1:])

    # Calculate the width of each bar
    bar_width = binwidth * (bins[1] - bins[0])

    # Plot the bar plot
    ax.bar(bin_centers, heights, width=bar_width, color='darkred', edgecolor='black', alpha=0.7)

plt.show()

"""#### Data Preparation"""

data.isnull().any()

data = data.dropna(how='any',axis=0)

# Checking Cardinality in the dataframe

cardinality = data.nunique()

# Print the cardinality for each column
cardinality

data['Customer Type'].unique()

data['Type of Travel'].unique()

data['Class'].unique()

data['Gender'].unique()

data['Satisfaction'].unique()

from sklearn.preprocessing import LabelEncoder

to_categorical_list = ['Customer Type','Type of Travel', 'Class', 'Gender', 'Satisfaction']
for i in to_categorical_list:
  data[i]=data[i].astype('category')

# Encoding
# Customer Type : 0:First-time 1:Returning
# Type of Travel : 0:Business 1:Personal
# Class : 0:Business 1:Economy 2:Economy Plus
# Gender : 0:Male 1:Female
# Satisfactiob : 0:Neutral or Dissatisfied 1:Satisfied

labelencoder = LabelEncoder()
for i in to_categorical_list:
  data[i] = labelencoder.fit_transform(data[i])

data.head()

# Correlation with Target Variable (Satisfaction here)
correlations = data.corrwith(data["Satisfaction"])

corr_df = pd.DataFrame(correlations, columns=['Correlation'])

plt.figure(figsize=(10, 6))
corr_df['Correlation'].plot(kind='bar')
plt.title('Correlation with Satisfaction')

plt.show()

# Based on Cardinality and Correlation
new_df = data.drop(['Gender', 'Age', 'Customer Type', 'Departure Delay', 'Arrival Delay', 'Gate Location', 'Flight Distance'], axis=1)
new_df.head()

plt.figure(figsize=(20, 18))
dataCol = new_df.columns

for i in range(len(dataCol)):
    plt.subplot(4, 5,i+1)
    plt.title(dataCol[i])
    sns.countplot(x=data[dataCol[i]],hue=data['Satisfaction'])

plt.tight_layout()
plt.show()

"""#### Train Test Split"""

y = new_df['Satisfaction']
X = new_df.drop(['Satisfaction'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)

X_train

y_test

"""#### Model Training"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)

y_pred_rfc = rfc.predict(X_test)

print(y_pred_rfc)
# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
# n_scores = cross_val_score(rfc, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# # report performance
# print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))

# accuracy_rfc = accuracy_score(y_test, y_pred_rfc)
# print("Random Forest Classifier Accuracy: ", accuracy_rfc)

# classification_report_rfc = classification_report(y_test, y_pred_rfc)
# print("Random Forest Classifier Classification Report: \n", classification_report_rfc)

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

y_pred_dtc = dtc.predict(X_test)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score(dtc, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
# report performance
print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))

accuracy_dtc = accuracy_score(y_test, y_pred_dtc)
print("Decision Tree Classifier Accuracy: ", accuracy_dtc)

classification_report_dtc = classification_report(y_test, y_pred_dtc)
print("Decision Tree Classifier Classification Report: \n", classification_report_dtc)

